name: kafka-poc
services:
  # ----------------------------
  # KRaft Controllers (3)
  # ----------------------------
  controller-1:
    image: apache/kafka:4.1.1
    container_name: controller-1
    restart: unless-stopped
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: controller
      KAFKA_LISTENERS: CONTROLLER://:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_METADATA_LOG_DIR: /var/lib/kafka/data
    volumes:
      - controller_1_data:/var/lib/kafka/data

  controller-2:
    image: apache/kafka:4.1.1
    container_name: controller-2
    restart: unless-stopped
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: controller
      KAFKA_LISTENERS: CONTROLLER://:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_METADATA_LOG_DIR: /var/lib/kafka/data
    volumes:
      - controller_2_data:/var/lib/kafka/data

  controller-3:
    image: apache/kafka:4.1.1
    container_name: controller-3
    restart: unless-stopped
    environment:
      KAFKA_NODE_ID: 3
      KAFKA_PROCESS_ROLES: controller
      KAFKA_LISTENERS: CONTROLLER://:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_METADATA_LOG_DIR: /var/lib/kafka/data
    volumes:
      - controller_3_data:/var/lib/kafka/data

  # ----------------------------
  # Brokers (3)
  # INTERNAL listener is for Docker network + inter-broker traffic
  # EXTERNAL is for your host machine (or your laptop via SSH)
  # ----------------------------
  broker-1:
    image: apache/kafka:4.1.1
    container_name: broker-1
    restart: unless-stopped
    depends_on:
      - controller-1
      - controller-2
      - controller-3
    ports:
      - "29092:9092"
    environment:
      KAFKA_NODE_ID: 4
      KAFKA_PROCESS_ROLES: broker
      KAFKA_LISTENERS: "PLAINTEXT://:19092,PLAINTEXT_HOST://:9092"
      # If you're on a remote VM, set PUBLIC_HOST to the VM IP/DNS (not localhost)
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://broker-1:19092,PLAINTEXT_HOST://172.16.232.50:29092"
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_METADATA_LOG_DIR: /var/lib/kafka/data
    volumes:
      - broker_1_data:/var/lib/kafka/data

  broker-2:
    image: apache/kafka:4.1.1
    container_name: broker-2
    restart: unless-stopped
    depends_on:
      - controller-1
      - controller-2
      - controller-3
    ports:
      - "39092:9092"
    environment:
      KAFKA_NODE_ID: 5
      KAFKA_PROCESS_ROLES: broker
      KAFKA_LISTENERS: "PLAINTEXT://:19092,PLAINTEXT_HOST://:9092"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://broker-2:19092,PLAINTEXT_HOST://172.16.232.50:39092"
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_METADATA_LOG_DIR: /var/lib/kafka/data
    volumes:
      - broker_2_data:/var/lib/kafka/data

  broker-3:
    image: apache/kafka:4.1.1
    container_name: broker-3
    restart: unless-stopped
    depends_on:
      - controller-1
      - controller-2
      - controller-3
    ports:
      - "49092:9092"
    environment:
      KAFKA_NODE_ID: 6
      KAFKA_PROCESS_ROLES: broker
      KAFKA_LISTENERS: "PLAINTEXT://:19092,PLAINTEXT_HOST://:9092"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://broker-3:19092,PLAINTEXT_HOST://172.16.232.50:49092"
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_METADATA_LOG_DIR: /var/lib/kafka/data
    volumes:
      - broker_3_data:/var/lib/kafka/data

  # ----------------------------
  # MongoDB (sink target for testing)
  # ----------------------------
  mongo:
    image: mongo:7
    container_name: mongo
    restart: unless-stopped
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db

  # ----------------------------
  # Download MongoDB Kafka Connector "all" (uber) JAR into a shared volume
  # Mongo docs recommend the "all" jar for Connect plugin installs. :contentReference[oaicite:4]{index=4}
  # ----------------------------
  connect-plugins:
    image: curlimages/curl:8.6.0
    container_name: connect-plugins
    restart: "no"
    command: >
      sh -lc "
        set -e;
        mkdir -p /plugins/mongodb;
        echo 'Downloading MongoDB Kafka Connector...';
        curl -fL
          -o /plugins/mongodb/mongo-kafka-connect-2.0.2-all.jar
          https://repo1.maven.org/maven2/org/mongodb/kafka/mongo-kafka-connect/2.0.2/mongo-kafka-connect-2.0.2-all.jar;
        echo 'Done.';
      "
    volumes:
      - connect_plugins:/plugins

  # ----------------------------
  # Kafka Connect (Distributed)
  # - Built-in FileStreamSinkConnector will write to ./lake as your "data lake" landing zone
  # - Mongo sink connector is available via the plugin we downloaded
  # ----------------------------
  connect:
    image: apache/kafka:4.1.1
    container_name: connect
    restart: unless-stopped
    depends_on:
      - broker-1
      - broker-2
      - broker-3
    ports:
      - "8083:8083"
    volumes:
      - ./connect-plugins:/opt/kafka/plugins
      - ./lake:/lake
      - ./connect/connect-distributed.properties:/opt/kafka/config/connect-distributed.properties
    command: >
      /opt/kafka/bin/connect-distributed.sh
      /opt/kafka/config/connect-distributed.properties

  minio:
    image: minio/minio:latest
    container_name: minio
    restart: unless-stopped
    ports:
      - "19000:9000"   # S3 API
      - "19001:9001"   # MinIO Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_DOMAIN: minio
    volumes:
      - ./minio-data:/data
    command: server /data --console-address ":9001"
    networks:
      default: {}
      lakehouse_net:
        aliases:
          - warehouse.minio
# ----------------------------
# Optional Kafka UI (handy but not required)
# ----------------------------
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: unless-stopped
    ports:
      - "7000:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker-1:19092
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: connect
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://connect:8083
    depends_on:
      - broker-1
      - connect
  
    # 2) Kafbat UI (same UI lineage, active fork) on 7001
  kafka-ui-kafbat:
    image: kafbat/kafka-ui:latest
    container_name: kafka-ui-kafbat
    restart: unless-stopped
    ports:
      - "7001:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker-1:19092
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: connect
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://connect:8083
      KAFKA_CLUSTERS_0_READONLY: "false"
      # Optional: enables editing cluster config from UI wizard (not required for topic create)
      DYNAMIC_CONFIG_ENABLED: "true"
    depends_on:
      - broker-1
      - connect

  # 3) Conduktor Console needs a DB (PostgreSQL is required)
  conduktor-postgresql:
    image: postgres:14
    container_name: conduktor-postgresql
    restart: unless-stopped
    environment:
      POSTGRES_DB: conduktor
      POSTGRES_USER: conduktor
      POSTGRES_PASSWORD: change_me
      POSTGRES_HOST_AUTH_METHOD: scram-sha-256
    volumes:
      - conduktor_pg_data:/var/lib/postgresql/data

  conduktor-console:
    image: conduktor/conduktor-console:latest
    container_name: conduktor-console
    restart: unless-stopped
    depends_on:
      - conduktor-postgresql
    ports:
      - "7002:8080"
    volumes:
      - conduktor_data:/var/conduktor
      - type: bind
        source: ./conduktor/platform-config.yaml
        target: /opt/conduktor/platform-config.yaml
        read_only: true
    environment:
      CDK_IN_CONF_FILE: /opt/conduktor/platform-config.yaml


#Spark + Iceberg REST + bucket init

  rest:
    image: apache/iceberg-rest-fixture
    container_name: rest
    restart: unless-stopped
    depends_on:
      - minio
    ports:
      - "8182:8181"
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
      CATALOG_WAREHOUSE: s3://warehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
    networks:
      - lakehouse_net

  mc:
    image: minio/mc
    container_name: mc
    depends_on:
      - minio
    restart: "no"
    networks:
      - lakehouse_net
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc alias set minio http://minio:9000 minioadmin minioadmin) do echo '...waiting for minio...' && sleep 1; done;
      /usr/bin/mc mb --ignore-existing minio/warehouse;
      tail -f /dev/null
      "

  spark-iceberg:
    image: tabulario/spark-iceberg
    container_name: spark-iceberg
    restart: unless-stopped
    depends_on:
      - rest
      - minio
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
    volumes:
      - ./scripts:/opt/spark/scripts
    ports:
      - "8888:8888"   # Jupyter notebooks (optional)
      - "18080:8080"  # Spark UI (mapped to 18080 to avoid surprises)
      - "10000:10000" # Spark Thrift server (optional)
      - "10001:10001"
    networks:
      - lakehouse_net
    entrypoint: >
      spark-submit
        --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.1,org.apache.hadoop:hadoop-aws:3.3.4,software.amazon.awssdk:bundle:2.24.8
        --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
        --conf spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
        --conf spark.sql.catalog.iceberg.type=hadoop
        --conf spark.sql.catalog.iceberg.warehouse=s3a://warehouse/iceberg
        --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
        --conf spark.hadoop.fs.s3a.access.key=minioadmin
        --conf spark.hadoop.fs.s3a.secret.key=minioadmin
        --conf spark.hadoop.fs.s3a.path.style.access=true
        --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false
        /opt/spark/scripts/bronze_assets_to_silver_assets.py
  
  spark-schema-infer:
    image: tabulario/spark-iceberg
    container_name: spark-schema-infer
    depends_on:
      - rest
      - minio
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
    volumes:
      - ./scripts:/opt/spark/scripts
    networks:
      - lakehouse_net
    entrypoint: >
      spark-submit
        --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.1,org.apache.hadoop:hadoop-aws:3.3.4,software.amazon.awssdk:bundle:2.24.8
        --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
        --conf spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
        --conf spark.sql.catalog.iceberg.type=hadoop
        --conf spark.sql.catalog.iceberg.warehouse=s3a://warehouse/iceberg
        --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
        --conf spark.hadoop.fs.s3a.access.key=minioadmin
        --conf spark.hadoop.fs.s3a.secret.key=minioadmin
        --conf spark.hadoop.fs.s3a.path.style.access=true
        --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false
        /opt/spark/scripts/schema_inferer.py

volumes:
  controller_1_data:
  controller_2_data:
  controller_3_data:
  broker_1_data:
  broker_2_data:
  broker_3_data:
  connect_plugins:
  mongo_data:
  conduktor_pg_data:
  conduktor_data:

networks:
  lakehouse_net: